<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" /> 

        <meta name="viewport" content="width=device-width, initial-scale=1">
        <style>
        img{
            display: block;
            margin-left: auto;
            margin-right: auto;
            }
         </style>

	    
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>Md Moniruzzaman</title>
        <link rel="icon" type="image/x-icon" href="assets/img/favicon.ico" />
        <!-- Font Awesome icons (free version)-->
        <script src="https://use.fontawesome.com/releases/v5.15.1/js/all.js" crossorigin="anonymous"></script>
        <!-- Google fonts-->
        <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:500,700" rel="stylesheet" type="text/css" />
        <link href="https://fonts.googleapis.com/css?family=Muli:400,400i,800,800i" rel="stylesheet" type="text/css" />


        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="css/styles.css" rel="stylesheet" />
    </head>
    <body id="page-top">
        <!-- Navigation-->
        <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
            <a class="navbar-brand js-scroll-trigger" href="#page-top">
                <span class="d-block d-lg-none">Md Moniruzzaman</span>
<!--                 <span class="d-none d-lg-block"><img class="img-fluid img-profile rounded-circle mx-auto mb-2" src="assets/img/profile.jpg" alt="" /></span> -->
                <span class="d-none d-lg-block"><img class="img-fluid img-profile mx-auto mb-2" src="assets/img/website.jpg" alt="" /></span>
            </a>
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button>
            <div class="collapse navbar-collapse" id="navbarSupportedContent">
                <ul class="navbar-nav">
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#about">About</a></li>
                    <!-- <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#education">Education</a></li> -->
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#research">Research</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#publications">Publications</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#Teaching">Teaching</a></li>
                    <!-- <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#skills">Skills</a></li> -->
                    <!-- <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#talk">Invited Talk</a></li> -->
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#awards">Awards</a></li>
                    <!-- <li class="nav-item"><a class="nav-link js-scroll-trigger" href="https://drive.google.com/file/d/1jcq1fAVDR_diUBetd99vsOKdnsZk8W2j/view" target="_blank">CV</a></li> -->
                </ul>
            </div>
        </nav>
        <!-- Page Content-->
        <div class="container-fluid p-0">
            <!-- About-->
            <section class="resume-section" id="about">
                <div class="resume-section-content">
                    <h1 class="mb-0">
                        Md
                        <span class="text-primary">Moniruzzaman</span>
                    </h1>
                    <div class="subheading mb-5">
                        Stony Brook University · New York 11794 · USA · (573) 476-4683 ·
                        <a href="mailto:mmoniruzzama@cs.stonybrook.edu">mmoniruzzama@cs.stonybrook.edu</a>
                    </div>                     
                      
                
                    <p class="lead mb-5">
		     I am a Ph.D. student in the Department of Computer Science at Stony Brook University working with <a href="https://www.cs.stonybrook.edu/people/faculty/ZhaozhengYin">Dr. Zhaozheng Yin</a>. I am interested in designing and implementing machine learning and deep learning-based algorithms to solve computer vision problems, such as image and video classification. In particular, I am familiar with video-based human action recognition, temporal action localization, future action anticipation, human pose estimation, and repetitive action counting with deep learning models.</p>				

                    <div class="social-icons">
                        <a href="https://www.linkedin.com/in/md-moniruzzaman-1730441a3/"><img src="assets\img\logo-linkin.png" height="39px" style="margin-bottom:-3px; margin-right: 10px"></a>
			<p> </p>
                        <a href="https://scholar.google.com/citations?user=aHJCt9oAAAAJ&hl=en"><img src="assets\img\logo-googlescholar.png" height="38px" style="margin-bottom:-3px; margin-right: 10px"></a>
                    </div>
                </div>
            </section>
            <hr class="m-0" />

            <!-- Research-->
            <section class="resume-section" id="research">
                <div class="resume-section-content">
                    <h2 class="mb-5">Research Projects</h2>
                        <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">3D Human Pose Reconstruction and Prediction from Wearable Sensors (<span style="color: brown">IEEE-JBHI</span> Minor Revision)</h3>
                            <div class="subheading mb-3"></div>
			    <!img src="assets\img\pose_model.png" alt="pose" width="1000" height="400">
			    <img src="assets\img\pose_model.png" alt="pose" style="width:90%">
			    <!img src="assets\img\pose_model.png" alt="pose" width="1000" height="400" style="width:100%">
                            <div class="subheading mb-3"></div>				
                            <p> <b>Innovation:</b> We introduce a novel Attention-Oriented Recurrent Neural Network (AttRNet) that contains a sensor-wise attention-oriented recurrent encoder, a reconstruction module, and a dynamic temporal attention-oriented recurrent decoder, to reconstruct the 3D human pose over time and predict the 3D human poses at the following time steps from the wearable IMU sensors and wearable cameras. </p>
                            <div class="subheading mb-3"></div>	
			    <img src="assets\img\obstacle.gif" alt="pred1" style="width:80%">
			    <p> </p>
			    <!img src="assets\img\obstacle.gif" alt="pred1" style="width:100%">
			    
			    <p> </p>
			    <img src="assets\img\stair.gif" alt="pred2" style="width:80%">
			    <p> </p>
                            <div class="subheading mb-3"></div>	
			    <div class="subsubheading mb-0"><b> This research is supported by National Science Foundation (NSF)</b></div>
                        </div>
                        <!-- <div class="flex-shrink-0"><span class="text-primary">July 2010 - December 2011</span></div> -->
                    </div>
            <hr>

<!--  -----------------------------------------------------------------------------------------------------------------------                    -->

            <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Feature Weakening, Contextualization, and Discrimination for Weakly Supervised Temporal Action Localization (<span style="color: brown">IEEE-TMM 2023</span>) </h3>
                            <div class="subheading mb-3"></div>
                            <!img src="assets\img\overview_tal.png" alt="localization" style="width:100%" width="1000" height="300">
			    <img src="assets\img\overview_tal.png" alt="localization" style="width:90%">
                            <div class="subheading mb-3"></div>
                            <p> <b>Innovation:</b> (1) We develop a Feature Weakening (FW) module for action completeness modeling; (2) We introduce a Feature Contextualization (FC) module to generate more representative contextualized features; and (3) We propose a Feature Discrimination (FD) module to highlight the most discriminative video segments/classes corresponding to each class/segment, respectively.</p>
                            <div class="subsubheading mb-0"><b> This research is supported by National Science Foundation (NSF)</b></div>
                        </div>
                        <!-- <div class="flex-shrink-0"><span class="text-primary">July 2010 - December 2011</span></div> -->
                    </div>
    <hr>	


<!--  -----------------------------------------------------------------------------------------------------------------------                    -->

            <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Collaborative Foreground, Background, and Action Modeling Network for Weakly Supervised Temporal Action Localization (<span style="color: brown">IEEE-TCSVT 2023</span>)</h3>
                            <div class="subheading mb-3"></div>
                            <!img src="assets\img\fba_net.png" alt="localization_tcsvt" width="1000" height="300" style="width:100%">
			    <img src="assets\img\fba_net.png" alt="localization_tcsvt" style="width:90%">
                            <div class="subheading mb-3"></div>
                            <p> <b>Innovation:</b> We introduce a novel collaborative foreground, background, and action Modeling Network (FBA-Net) that consists of a foreground modeling (FM) branch, the background modeling (BM) branch, and the class-specific action and background modeling (CM) branch. These branches collaborate with each other to localize both the discriminative and ambiguous action frames and suppress both the discriminative and ambiguous background frames.</p>
				
			    <div class="subsubheading mb-0"><b> This research is supported by National Science Foundation (NSF)</b></div>
                        </div>
                        <!-- <div class="flex-shrink-0"><span class="text-primary">July 2010 - December 2011</span></div> -->
                    </div>
    <hr>


<!--  -----------------------------------------------------------------------------------------------------------------------                    -->

            <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Jointly-Learnt Networks for Future Action Anticipation via Self-Knowledge Distillation and Cycle Consistency (<span style="color: brown">IEEE-TCSVT 2022</span>)</h3>
                            <div class="subheading mb-3"></div>
                            <img src="assets\img\anticipation.png" alt="anticipation_tcsvt" height ="500">
                            <div class="subheading mb-3"></div>
                            <p> <b>Innovation:</b> (a) We propose a Jointly-learnt Action Anticipation Network (J-AAN) that anticipates future actions from the observed past actions in both direct and recursive ways; (b) We utilize a Self-knowledge distillation mechanism to train the J-AAN, where the J-AAN gradually distills its own knowledge during the training to soften the hard labels to model the uncertainty of future action anticipation; and (c) We design Forward and backward J-AANs with cycle consistency, where the backward J-AAN evaluates how well the forward JAAN anticipates future actions by anticipating past actions from the anticipated future actions.</p>
				
			    <div class="subsubheading mb-0"><b> This research is supported by National Science Foundation (NSF)</b></div>
                        </div>
                        <!-- <div class="flex-shrink-0"><span class="text-primary">July 2010 - December 2011</span></div> -->
                    </div>
    <hr>

<!--  -----------------------------------------------------------------------------------------------------------------------                    -->

            <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Human Action Recognition by Discriminative Feature Pooling and Video Segmentation Attention Model (<span style="color: brown">IEEE-TMM 2021</span>)</h3>
                            <div class="subheading mb-3"></div>
                            <img src="assets\img\overview_ar.png" alt="recognition" style="width:90%">
                            <div class="subheading mb-3"></div>
                            <p><b> Abstract: </b> We introduce a simple yet effective network that embeds a novel Discriminative Feature Pooling (DFP) mechanism and a novel Video Segment Attention Model (VSAM), for video-based human action recognition from both trimmed and untrimmed videos. Our DFP module introduces an attentional pooling mechanism for 3D Convolutional Neural Networks that attentionally pools 3D convolutional feature maps to emphasize the most critical spatial, temporal, and channel-wise features related to the actions within a video segment, while our VSAM ensembles these most critical features from all video segments and learns (1) class-specific attention weights to classify the video segments into the corresponding action categories, and (2) class-agnostic attention weights to rank the video segments based on their relevance to the action class. Our action recognition network can be trained from both trimmed videos in a fully-supervised way and untrimmed videos in a weakly-supervised way. For untrimmed videos with weak labels, our network learns attention weights without the requirement of precise temporal annotations of action occurrences in videos. Evaluated on the untrimmed video datasets of THUMOS14 and ActivityNet1.2, and trimmed video datasets of HMDB51, UCF101, and HOLLYWOOD2, our network achieves promising performance, compared to the latest state-of-the-art methods.</p>
                            	<div class="subsubheading mb-0"><b> This research is supported by National Science Foundation (NSF)</b></div>
                        </div>
                        <!-- <div class="flex-shrink-0"><span class="text-primary">July 2010 - December 2011</span></div> -->
                    </div>
    <hr>

<!--  -----------------------------------------------------------------------------------------------------------------------                    -->

            <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Action Completeness Modeling with Background Aware Networks for weakly-Supervised Temporal Action Localization (<span style="color: brown">ACMMM 2020</span>)</h3>
                            <div class="subheading mb-3"></div>
                            <!img src="assets\img\banet.png" alt="localization_acmmm" style="width:100%" width = "1000" height ="350">
			    <img src="assets\img\banet.png" alt="localization_acmmm" style="width:90%">
                            <div class="subheading mb-3"></div>
                            <p> <b>Abstract:</b> The state-of-the-art of fully-supervised methods for temporal action localization from untrimmed videos has achieved impressive results. Yet, it remains unsatisfactory for the weakly-supervised temporal action localization, where only video-level action labels are given without the timestamp annotation on when the actions occur. The main reason comes from that, the weakly-supervised networks only focus on the highly discriminative frames, but there are some ambiguous frames in both background and action classes. The ambiguous frames in the background class are very similar to the real actions, which may be treated as target actions and result in false positives. On the other hand, the ambiguous frames in the action class which possibly contain action instances, are prone to be false negatives by the weakly-supervised networks and result in a coarse localization. To solve these problems, we introduce a novel weakly-supervised Action Completeness Modeling with Background Aware Networks (ACM-BANets). Our Background Aware Network (BANet) contains a weight-sharing two-branch architecture, with an action-guided Background aware Temporal Attention Module (B-TAM) and an asymmetrical training strategy, to suppress both highly discriminative and ambiguous background frames to remove the false positives. Our action completeness modeling contains multiple BANets, and the BANets are forced to discover different but complementary action instances to completely localize the action instances in both highly discriminative and ambiguous action frames. In the 𝑖-th iteration, the 𝑖-th BANet discovers the discriminative features, which are then erased from the feature map. The partially-erased feature map is fed into the (𝑖 + 1)-th BANet of the next iteration to force this BANet to discover discriminative features different from the 𝑖-th BANet. Evaluated on two challenging untrimmed video datasets, THUMOS14 and ActivityNet1.3, our approach outperforms all the current weakly-supervised methods for temporal action localization.</p>
				<div class="subsubheading mb-0"><b> This research is supported by National Science Foundation (NSF)</b></div>
                        </div>
                        <!-- <div class="flex-shrink-0"><span class="text-primary">July 2010 - December 2011</span></div> -->
                    </div>
    <hr>
			
<!--   --------------------------------------------------------------------------------------------------------------------------                   -->

                    <div class="d-flex flex-column flex-md-row justify-content-between">
                        <div class="flex-grow-1">
                            <h3 class="mb-0"> Spatial Attention Mechanism for Weakly Supervised Fire and Traffic Accident Scene Classification (<span style="color: brown">SMARTCOMP 2019</span>)</h3>
                            <div class="subheading mb-3"></div>
                            <img src="assets\img\scene_1.gif" alt="accident1" style="width:70%">
			    <p> </p>
			    <img src="assets\img\scene_2.gif" alt="accident2" style="width:70%">
                            <div class="subheading mb-3"></div>
                            <!-- <div class="subheading mb-3">Shout! Media Productions</div> -->
                            <p><b> Abstract: </b> During the past ten years, on average there were near 16.5 thousand of hazardous materials (hazmat) transport incidents per year resulting in $82 million of damages. Prompt, accurate, objective assessment on hazmat incidents is important for the first responders to take appropriate actions timely, which will reduce the damage of hazmat incidents and protect the safety of people and the environment. Therefore, one of the most important steps is to automatically detect transport incidents, such as fire and traffic accidents. In this paper, we introduce a simple and yet effective framework that integrates the convolutional feature maps of deep Convolutional Neural Networks with a spatial attention mechanism for fire and traffic accident scene classification. Our spatial attention model learns to highlight the most discriminative convolutional features, which is related to the regions of interest in the input image. We train our network in a weakly supervised way. In other words, without the requirement of a precise bounding box annotating the exact location of fire or traffic accidents in the image, our network can be learned from the only image-level label. In addition to the image-based traffic scene classification, the model is also applied on a set of collected videos for real-world applications. The proposed model, a simple end-to-end architecture, achieves promising performance on fire scene classification from images, and traffic accident scene classification from both images and videos.</p>
			    <div class="subsubheading mb-0"><b> This research is supported by Mid-America Transportation Center (MATC), and Intelligent Systems Center (ISC) at Missouri University of Science and Technology.</b></div>
                            <div class="subheading mb-5"></div>

                        </div>
                        <!-- <div class="flex-shrink-0"><span class="text-primary">September 2008 - June 2010</span></div> -->
                    </div>

                  </div>

            </section>
            <hr class="m-0" />


            <!-- Publications-->
            <section class="resume-section" id="publications">
                <div class="resume-section-content">
                    <h2 class="mb-5">Publications</h2>
                    <p> 1. <b>Md Moniruzzaman</b>, Zhaozheng Yin, Md Sanzid Bin Hossain, Hwan Choi, and Zhishan Guo. “Wearable Motion Capture: Reconstructing and Predicting 3D Human Poses from Wearable Sensors.” IEEE Journal of Biomedical and Health Informatics <b>(IEEE-JBHI)</b>, 2023 (Minor Revision).</p>
		    <p> 2. <b>Md Moniruzzaman</b>, and Zhaozheng Yin. “Feature Weakening, Contextualization, and Discrimination for Weakly Supervised Temporal Action Localization.” IEEE Transactions on Multimedia <b>(IEEE-TMM)</b>, Accepted, 2023. (Impact Factor: 8.182)</p>
		    <p> 3. <b>Md Moniruzzaman</b>, and Zhaozheng Yin. “Collaborative Foreground, Background, and Action Modeling Network for Weakly Supervised Temporal Action Localization.” IEEE Transactions on Circuits and Systems for Video Technology <b>(IEEE-TCSVT)</b>, Accepted, 2023. (Impact Factor: 5.859)</p>
		    <p> 4. <b>Md Moniruzzaman</b>, Md Moniruzzaman, Zhaozheng Yin, Zhihai He, Ruwen Qin, and Ming C Leu. “Jointly-Learnt Networks for Future Action Anticipation via Self-Knowledge Distillation and Cycle Consistency.” IEEE Transactions on Circuits and Systems for Video Technology <b>(IEEE-TCSVT)</b>, 2022. (Impact Factor: 5.859)</p>
		    <p> 5. <b>Md Moniruzzaman</b>, Zhaozheng Yin, Zhihai He, Ruwen Qin, and Ming C Leu. “Human Action Recognition by Discriminative Feature Pooling and Video Segmentation Attention Model.” IEEE Transactions on Multimedia <b>(IEEE-TMM)</b>, 2021. (Impact Factor: 8.182)</p>
		    <p> 6. Md Al-Amin, Ruwen Qin, <b>Md Moniruzzaman</b>, Zhaozheng Yin, Wenjin Tao, and Ming C Leu. “An individualized system of skeletal data-based CNN classifiers for action recognition in manufacturing assembly.” Journal of Intelligent Manufacturing, 2021.
		    <p> 7. <b>Md Moniruzzaman</b>, Zhaozheng Yin, Zhihai He, Ruwen Qin, and Ming C Leu. “Action Completeness Modeling with Background Aware Networks for Weakly-Supervised Temporal Action Localization.” In Proceedings of the 28th ACM International Conference on Multimedia <b>(ACM-MM)</b>, 2020.</p>
		    <p> 8. <b>Md Moniruzzaman</b>, Zhaozheng Yin, and Ruwen Qin. “Spatial Attention Mechanism for Weakly Supervised Fire and Traffic Accident Scene Classification.” In 2019 IEEE International Conference on Smart Computing, 2019.</p>
                </div>
            </section>
            <hr class="m-0" />


            <!-- Awards-->
            <section class="resume-section" id="awards">
                <div class="resume-section-content">
                    <h2 class="mb-5">Awards & Certifications</h2>
                    <ul class="fa-ul mb-0">
                        <li>
                            <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                            Academic Achievement Award, Department of Computer Science, Missouri University of Science and Technology, 2019
                        </li>
                        <li>
                            <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                            NSF Travel Grant Award in IEEE International Conference on Smart Computing, 2019.
                        </li>
	                <li>
                            <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                            Academic Achievement Award, Department of Computer Science, Missouri University of Science and Technology, 2018.
                        </li>
                        <li>
                            <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                            3
                            <sup>rd</sup>
                            Place - Best Poster Award, Poster Competition, Intelligent Systems Center (ISC), 2018.
                        </li>
                        <li>
                            <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                            2
                            <sup>nd</sup>
                            Place - Best Paper Award, Graduate Research Symposium, Intelligent Systems Center (ISC), 2018.
                        </li>
                    </ul>
                </div>
            </section>
            </section>
            <hr class="m-0" />


		

</html>
