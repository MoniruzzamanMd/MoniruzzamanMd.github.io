<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" /> 

        <meta name="viewport" content="width=device-width, initial-scale=1">
        <style>
        img{
            display: block;
            margin-left: auto;
            margin-right: auto;
            }
         </style>

	    
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>Md Moniruzzaman</title>
        <link rel="icon" type="image/x-icon" href="assets/img/favicon.ico" />
        <!-- Font Awesome icons (free version)-->
        <script src="https://use.fontawesome.com/releases/v5.15.1/js/all.js" crossorigin="anonymous"></script>
        <!-- Google fonts-->
        <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:500,700" rel="stylesheet" type="text/css" />
        <link href="https://fonts.googleapis.com/css?family=Muli:400,400i,800,800i" rel="stylesheet" type="text/css" />


        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="css/styles.css" rel="stylesheet" />
    </head>
    <body id="page-top">
        <!-- Navigation-->
        <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
            <a class="navbar-brand js-scroll-trigger" href="#page-top">
                <span class="d-block d-lg-none">Md Moniruzzaman</span>
<!--                 <span class="d-none d-lg-block"><img class="img-fluid img-profile rounded-circle mx-auto mb-2" src="assets/img/profile.jpg" alt="" /></span> -->
                <span class="d-none d-lg-block"><img class="img-fluid img-profile mx-auto mb-2" src="assets/img/website.jpg" alt="me"/></span>
            </a>
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button>
            <div class="collapse navbar-collapse" id="navbarSupportedContent">
                <ul class="navbar-nav">
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#about">About</a></li>
                    <!-- <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#education">Education</a></li> -->
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#research">Research</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#publications">Publications</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#courseworks">Courseworks</a></li>
                    <!-- <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#skills">Skills</a></li> -->
                    <!-- <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#talk">Invited Talk</a></li> -->
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#awards">Awards</a></li>
                    <!-- <li class="nav-item"><a class="nav-link js-scroll-trigger" href="https://drive.google.com/file/d/1jcq1fAVDR_diUBetd99vsOKdnsZk8W2j/view" target="_blank">CV</a></li> -->
                </ul>
            </div>
        </nav>
        <!-- Page Content-->
        <div class="container-fluid p-0">
            <!-- About-->
            <section class="resume-section" id="about">
                <div class="resume-section-content">
                    <h1 class="mb-0">
                        Md
                        <span class="text-primary">Moniruzzaman</span>
                    </h1>
                    <div class="subheading mb-5">
                        Stony Brook University · New York 11794 · USA ·
                        <a href="mailto:mmoniruzzama@cs.stonybrook.edu">mmoniruzzama@cs.stonybrook.edu</a>
                    </div>                     
                      
                
                    <p class="lead mb-5">
		     I am currently a <b>Ph.D. student</b> in the Department of Computer Science at Stony Brook University. I am a <b>Research Assistant</b> at the Computer Vision and Biomedical Imaging lab under the supervision of <a href="https://www.cs.stonybrook.edu/people/faculty/ZhaozhengYin">Dr. Zhaozheng Yin</a>.</p>	

		    <p class="lead mb-5">
			    During my Ph.D., I was a <b>Research Intern</b> at <b>Peloton Interactive Inc.</b>, where I worked with experts from Peloton on human activity recognition projects. I received my Bachelor of Science (B.Sc) degree in Electronics and Communication Engineering from Khulna University of Engineering Technology, Bangladesh.</p>
		     	
	            <p class="lead mb-5">
			    <b>Research Interest:</b> I am broadly interested in designing and implementing <span style="color: brown"><b>machine learning</b></span> and <span style="color: brown"><b>deep learning</b></span>-based algorithms to solve computer vision problems, such as image and video classification. In particular, I am familiar with <span style="color: blue"><b>video-based human action recognition, temporal action localization, future action anticipation, human pose estimation, online action detection, and repetitive action counting with deep learning models</b></span>. During my Ph.D., many of my research ideas have been accepted to top-tier journals and conference such as IEEE-TMM, IEEE-TCSVT, IEEE-JBHI, and ACMMM.</p> 
                   
		     <div class="social-icons">
                        <a href="https://www.linkedin.com/in/md-moniruzzaman-1730441a3/"><img src="assets\img\logo-linkin.png" height="39px" style="margin-bottom:-3px; margin-right: 10px"></a>
			<p> </p>
                        <a href="https://scholar.google.com/citations?user=aHJCt9oAAAAJ&hl=en"><img src="assets\img\logo-googlescholar.png" height="38px" style="margin-bottom:-3px; margin-right: 10px"></a>
                    </div>
                </div>
            </section>
            <hr class="m-0" />

            <!-- Research-->
		
            <section class="resume-section" id="research">
                <div class="resume-section-content">
                    <h2 class="mb-5">Research Projects</h2>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Human Action Recognition</h3>
		            <p> <span style="color: brown"><b>Our Proposal: Human Action Recognition by Discriminative Feature Pooling and Video Segmentation Attention Model (IEEE-TMM 2021)</span></b></p>	    		
                            <div class="subheading mb-3"></div>
                            <img src="assets\img\overview_ar.png" alt="recognition" style="width:75%">
                            <div class="subheading mb-3"></div>
                            <p><b> Innovation: </b> We introduce a simple yet effective network that embeds a novel Discriminative Feature Pooling (DFP) mechanism and a novel Video Segment Attention Model (VSAM), for video-based human action recognition from both trimmed and untrimmed videos. Our DFP module emphasizes the most critical spatial, temporal, and channel-wise features related to the actions within a video segment, while our VSAM emphasizes the most representative features across the video segments.</p>
                            <img src="assets\img\recognition_results.gif" alt="recognition_results" style="width:75%">	
				<div class="subsubheading mb-0"><b> This research is supported by the National Science Foundation (NSF)</b></div>
                        </div>
                        <!-- <div class="flex-shrink-0"><span class="text-primary">July 2010 - December 2011</span></div> -->
                    </div>
    <hr>

<!--  -----------------------------------------------------------------------------------------------------------------------                    -->

            <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
			    <h3 class="mb-0">Temporal Action Localization</h3>
			    <p> <span style="color: brown"><b>Our Proposal: Feature Weakening, Contextualization, and Discrimination for Weakly Supervised Temporal Action Localization (IEEE-TMM 2023)</span></b></p>
                            <div class="subheading mb-3"></div>
                            <!img src="assets\img\overview_tal.png" alt="localization" style="width:100%" width="1000" height="300">
			    <img src="assets\img\overview_tal.png" alt="localization" style="width:75%">
                            <div class="subheading mb-3"></div>
                            <p> <b>Innovation:</b> (1) We develop a Feature Weakening (FW) module for action completeness modeling; (2) We introduce a Feature Contextualization (FC) module to generate more representative contextualized features; and (3) We propose a Feature Discrimination (FD) module to highlight the most discriminative video segments/classes corresponding to each class/segment, respectively.</p>
                            <img src="assets\img\f3_net_results.gif" alt="localization_results" style="width:75%">
			    <div class="subsubheading mb-0"><b> This research is supported by the National Science Foundation (NSF)</b></div>
                        </div>
                        <!-- <div class="flex-shrink-0"><span class="text-primary">July 2010 - December 2011</span></div> -->
                    </div>
    <hr>	


<!--  -----------------------------------------------------------------------------------------------------------------------                    -->

            <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <p> <span style="color: brown"><b>Our Proposal: Collaborative Foreground, Background, and Action Modeling Network for Weakly Supervised Temporal Action Localization (IEEE-TCSVT 2023)</span></b></p>
                            <div class="subheading mb-3"></div>
                            <!img src="assets\img\fba_net.png" alt="localization_tcsvt" width="1000" height="300" style="width:100%">
			    <img src="assets\img\fba_net.gif" alt="localization_tcsvt" style="width:75%">
                            <div class="subheading mb-3"></div>
                            <p> <b>Innovation:</b> We introduce a novel collaborative foreground, background, and action Modeling Network (FBA-Net) that consists of a foreground modeling (FM) branch, the background modeling (BM) branch, and the class-specific action and background modeling (CM) branch. These branches collaborate with each other to localize both the discriminative and ambiguous action frames and suppress both the discriminative and ambiguous background frames.</p>
				
			    <div class="subsubheading mb-0"><b> This research is supported by the National Science Foundation (NSF)</b></div>
                        </div>
                        <!-- <div class="flex-shrink-0"><span class="text-primary">July 2010 - December 2011</span></div> -->
                    </div>
    <hr>

<!--  -----------------------------------------------------------------------------------------------------------------------                    -->			
            <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
			    <p> <span style="color: brown"><b>Our Proposal: Action Completeness Modeling with Background Aware Networks for weakly-Supervised Temporal Action Localization (ACMMM 2020)</span></b></p>
                            <div class="subheading mb-3"></div>
                            <!img src="assets\img\banet.png" alt="localization_acmmm" style="width:100%" width = "1000" height ="350">
			    <img src="assets\img\acm_goal.gif" alt="localization_acmmm" style="width:75%">
                            <div class="subheading mb-3"></div>
                            <p> <b>Innovation:</b> (1) We design a novel Background Aware Network (BANet) to suppress both highly discriminative and ambiguous background frames to significantly reduce the false positive rate; and (2) We propose an action completeness modeling framework that contains multiple BANets, where the BANets are forced to localize different but complementary action instances in both highly discriminative and ambiguous action frames.</p>
				<div class="subsubheading mb-0"><b> This research is supported by the National Science Foundation (NSF)</b></div>
                        </div>
                        <!-- <div class="flex-shrink-0"><span class="text-primary">July 2010 - December 2011</span></div> -->
                    </div>
    <hr>
	

<!--  -----------------------------------------------------------------------------------------------------------------------                    -->

            <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Future Action Anticipation</h3>
			    <p> <span style="color: brown"><b>Our Proposal: Jointly-Learnt Networks for Future Action Anticipation via Self-Knowledge Distillation and Cycle Consistency (IEEE-TCSVT 2022)</span></b></p>	
                            <div class="subheading mb-3"></div>
                            <img src="assets\img\anticipation_overview.png" alt="anticipation_tcsvt" style="width:75%">
                            <div class="subheading mb-3"></div>
                            <p> <b>Innovation:</b> (a) We propose a Jointly-learnt Action Anticipation Network (J-AAN) that anticipates future actions from the observed past actions in both direct and recursive ways; (b) We utilize a Self-knowledge distillation mechanism to train the J-AAN, where the J-AAN gradually distills its own knowledge during the training to soften the hard labels to model the uncertainty of future action anticipation; and (c) We design Forward and backward J-AANs with cycle consistency, where the backward J-AAN evaluates how well the forward JAAN anticipates future actions by anticipating past actions from the anticipated future actions.</p>
				
			    <div class="subsubheading mb-0"><b> This research is supported by the National Science Foundation (NSF)</b></div>
                        </div>
                        <!-- <div class="flex-shrink-0"><span class="text-primary">July 2010 - December 2011</span></div> -->
                    </div>
    <hr>

<!--  -----------------------------------------------------------------------------------------------------------------------                    -->

                <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Human Pose Reconstruction and Prediction</h3>
			    <p> <span style="color: brown"><b>Our Proposal: Wearable Motion Capture: Reconstructing and Predicting 3D Human Poses from Wearable Sensors (IEEE-JBHI 2023)</span></b></p>
                            <div class="subheading mb-3"></div>
			    <!img src="assets\img\pose_model.png" alt="pose" width="1000" height="400">
			    <img src="assets\img\pose_proposal.png" alt="pose" style="width:40%">
			    <!img src="assets\img\pose_model.png" alt="pose" width="1000" height="400" style="width:100%">
                            <div class="subheading mb-3"></div>				
                            <p> <b>Innovation:</b> We introduce a novel Attention-Oriented Recurrent Neural Network (AttRNet) that contains a sensor-wise attention-oriented recurrent encoder, a reconstruction module, and a dynamic temporal attention-oriented recurrent decoder, to reconstruct the 3D human pose over time and predict the 3D human poses at the following time steps from the wearable IMU sensors and wearable cameras. </p>
                            <div class="subheading mb-3"></div>	
			    <img src="assets\img\obstacle.gif" alt="pred1" style="width:60%">
			    <p> </p>
			    <!img src="assets\img\obstacle.gif" alt="pred1" style="width:100%">
			    
			    <p> </p>
			    <img src="assets\img\stair.gif" alt="pred2" style="width:60%">
			    <p> </p>
                            <div class="subheading mb-3"></div>	
			    <div class="subsubheading mb-0"><b> This research is supported by the National Science Foundation (NSF)</b></div>
                        </div>
                        <!-- <div class="flex-shrink-0"><span class="text-primary">July 2010 - December 2011</span></div> -->
                    </div>
            <hr>

<!--  -----------------------------------------------------------------------------------------------------------------------                    -->

                    <div class="d-flex flex-column flex-md-row justify-content-between">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Fire and Traffic Accident Scene Classification</h3>
			    <p> <span style="color: brown"><b>Our Proposal: Spatial Attention Mechanism for Weakly Supervised Fire and Traffic Accident Scene Classification (SMARTCOMP 2019)</span></b></p>	
                            <div class="subheading mb-3"></div>
                            <img src="assets\img\scene_1.gif" alt="accident1" style="width:50%">
			    <p> </p>
			    <img src="assets\img\scene_2.gif" alt="accident2" style="width:50%">
                            <div class="subheading mb-3"></div>
                            <!-- <div class="subheading mb-3">Shout! Media Productions</div> -->
                            <p><b> Innovation: </b> we introduce a simple yet effective framework that integrates the convolutional feature maps of deep Convolutional Neural Networks with a spatial attention mechanism for fire and traffic accident scene classification. In addition to the image-based traffic scene classification, the model is also applied on a set of collected videos for real-world applications.</p>
				
			    <div class="subsubheading mb-0"><b> This research is supported by the Mid-America Transportation Center (MATC), and Intelligent Systems Center (ISC) at Missouri University of Science and Technology.</b></div>
                            <div class="subheading mb-5"></div>

                        </div>
                        <!-- <div class="flex-shrink-0"><span class="text-primary">September 2008 - June 2010</span></div> -->
                    </div>

                  </div>

            </section>
            <hr class="m-0" />


            <!-- Publications-->
            <section class="resume-section" id="publications">
                <div class="resume-section-content">
                    <h2 class="mb-5">Publications</h2>
                    <p> 1. <b>Md Moniruzzaman</b>, Zhaozheng Yin, Md Sanzid Bin Hossain, Hwan Choi, and Zhishan Guo. “Wearable Motion Capture: Reconstructing and Predicting 3D Human Poses from Wearable Sensors.” IEEE Journal of Biomedical and Health Informatics <b>(IEEE-JBHI)</b>, 2023 (Minor Revision).</p>
		    <p> 2. <b>Md Moniruzzaman</b>, and Zhaozheng Yin. “Feature Weakening, Contextualization, and Discrimination for Weakly Supervised Temporal Action Localization.” IEEE Transactions on Multimedia <b>(IEEE-TMM)</b>, Accepted, 2023. (Impact Factor: 8.182)</p>
		    <p> 3. <b>Md Moniruzzaman</b>, and Zhaozheng Yin. “Collaborative Foreground, Background, and Action Modeling Network for Weakly Supervised Temporal Action Localization.” IEEE Transactions on Circuits and Systems for Video Technology <b>(IEEE-TCSVT)</b>, Accepted, 2023. (Impact Factor: 5.859)</p>
		    <p> 4. <b>Md Moniruzzaman</b>, Md Moniruzzaman, Zhaozheng Yin, Zhihai He, Ruwen Qin, and Ming C Leu. “Jointly-Learnt Networks for Future Action Anticipation via Self-Knowledge Distillation and Cycle Consistency.” IEEE Transactions on Circuits and Systems for Video Technology <b>(IEEE-TCSVT)</b>, 2022. (Impact Factor: 5.859)</p>
		    <p> 5. <b>Md Moniruzzaman</b>, Zhaozheng Yin, Zhihai He, Ruwen Qin, and Ming C Leu. “Human Action Recognition by Discriminative Feature Pooling and Video Segmentation Attention Model.” IEEE Transactions on Multimedia <b>(IEEE-TMM)</b>, 2021. (Impact Factor: 8.182)</p>
		    <p> 6. Md Al-Amin, Ruwen Qin, <b>Md Moniruzzaman</b>, Zhaozheng Yin, Wenjin Tao, and Ming C Leu. “An individualized system of skeletal data-based CNN classifiers for action recognition in manufacturing assembly.” Journal of Intelligent Manufacturing, 2021.
		    <p> 7. <b>Md Moniruzzaman</b>, Zhaozheng Yin, Zhihai He, Ruwen Qin, and Ming C Leu. “Action Completeness Modeling with Background Aware Networks for Weakly-Supervised Temporal Action Localization.” In Proceedings of the 28th ACM International Conference on Multimedia <b>(ACM-MM)</b>, 2020.</p>
		    <p> 8. <b>Md Moniruzzaman</b>, Zhaozheng Yin, and Ruwen Qin. “Spatial Attention Mechanism for Weakly Supervised Fire and Traffic Accident Scene Classification.” In 2019 IEEE International Conference on Smart Computing, 2019.</p>
                </div>
            </section>
            <hr class="m-0" />

            <!-- Coursework-->
            <section class="resume-section" id="courseworks">
                <div class="resume-section-content">
                    <h2 class="mb-5">Courseworks</h2>
		    <p> - Analysis of Algorithms </p>
		    <p> - Advanced Topics in Data Mining </p>
		    <p> - Applied Spatial and Temporal Data Analysis </p>
		    <p> - Clustering Algorithm </p>
		    <p> - Computing with Logic </p>
		    <p> - Deep Learning Neural Networks </p>
                    <p> - Introduction to Machine Learning </p>
		    <p> - Introduction to Computer Vision </p>
                    <p> - Introduction to Deep Learning </p>
		    <p> - Machine Learning in Computer Vision </p>
                    <p> - Theory of Database Systems </p>    
            </section>
            <hr class="m-0" />


            <!-- Awards-->
            <section class="resume-section" id="awards">
                <div class="resume-section-content">
                    <h2 class="mb-5">Awards & Certifications</h2>
                    <ul class="fa-ul mb-0">
                        <li>
                            <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                            Academic Achievement Award, Department of Computer Science, Missouri University of Science and Technology, 2019.
                        </li>
                        <li>
                            <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                            NSF Travel Grant Award in IEEE International Conference on Smart Computing, 2019.
                        </li>
	                <li>
                            <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                            Academic Achievement Award, Department of Computer Science, Missouri University of Science and Technology, 2018.
                        </li>
                        <li>
                            <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                            3
                            <sup>rd</sup>
                            Place - Best Poster Award, Poster Competition, Intelligent Systems Center (ISC), 2018.
                        </li>
                        <li>
                            <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                            2
                            <sup>nd</sup>
                            Place - Best Paper Award, Graduate Research Symposium, Intelligent Systems Center (ISC), 2018.
                        </li>
                    </ul>
                </div>
            </section>
            </section>
            <hr class="m-0" />

        <!-- Bootstrap core JS-->
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/js/bootstrap.bundle.min.js"></script>
        <!-- Third party plugin JS-->
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.4.1/jquery.easing.min.js"></script>
        <!-- Core theme JS-->
        <script src="js/scripts.js"></script>
    </body>
</html>
