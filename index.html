<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" /> 

        <meta name="viewport" content="width=device-width, initial-scale=1">
        <style>
        img{
            display: block;
            margin-left: auto;
            margin-right: auto;
            }
         </style>

	    
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>Md Moniruzzaman</title>
        <link rel="icon" type="image/x-icon" href="assets/img/favicon.ico" />
        <!-- Font Awesome icons (free version)-->
        <script src="https://use.fontawesome.com/releases/v5.15.1/js/all.js" crossorigin="anonymous"></script>
        <!-- Google fonts-->
        <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:500,700" rel="stylesheet" type="text/css" />
        <link href="https://fonts.googleapis.com/css?family=Muli:400,400i,800,800i" rel="stylesheet" type="text/css" />


        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="css/styles.css" rel="stylesheet" />
    </head>
    <body id="page-top">
        <!-- Navigation-->
        <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
            <a class="navbar-brand js-scroll-trigger" href="#page-top">
                <span class="d-block d-lg-none">Md Moniruzzaman</span>
<!--                 <span class="d-none d-lg-block"><img class="img-fluid img-profile rounded-circle mx-auto mb-2" src="assets/img/profile.jpg" alt="" /></span> -->
                <span class="d-none d-lg-block"><img class="img-fluid img-profile mx-auto mb-2" src="assets/img/website.jpg" alt="" /></span>
            </a>
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button>
            <div class="collapse navbar-collapse" id="navbarSupportedContent">
                <ul class="navbar-nav">
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#about">About</a></li>
                    <!-- <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#education">Education</a></li> -->
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#research">Research</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#publications">Publications</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#Teaching">Teaching</a></li>
                    <!-- <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#skills">Skills</a></li> -->
                    <!-- <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#talk">Invited Talk</a></li> -->
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#awards">Awards</a></li>
                    <!-- <li class="nav-item"><a class="nav-link js-scroll-trigger" href="https://drive.google.com/file/d/1jcq1fAVDR_diUBetd99vsOKdnsZk8W2j/view" target="_blank">CV</a></li> -->
                </ul>
            </div>
        </nav>
        <!-- Page Content-->
        <div class="container-fluid p-0">
            <!-- About-->
            <section class="resume-section" id="about">
                <div class="resume-section-content">
                    <h1 class="mb-0">
                        Md
                        <span class="text-primary">Moniruzzaman</span>
                    </h1>
                    <div class="subheading mb-5">
                        Stony Brook University · New York 11794 · USA · (573) 476-4683 ·
                        <a href="mailto:mmoniruzzama@cs.stonybrook.edu">mmoniruzzama@cs.stonybrook.edu</a>
                    </div>                     
                      
                
                    <p class="lead mb-5">
		     I am a Ph.D. student in the Department of Computer Science at Stony Brook University working with <a href="https://www.cs.stonybrook.edu/people/faculty/ZhaozhengYin">Dr. Zhaozheng Yin</a>. I am interested in designing and implementing machine learning and deep learning-based algorithms to solve computer vision problems, such as image and video classification. In particular, I am familiar with video-based human action recognition, temporal action localization, future action anticipation, human pose estimation, and repetitive action counting with deep learning models.</p>				

                    <div class="social-icons">
                        <a href="https://www.linkedin.com/in/md-moniruzzaman-1730441a3/"><img src="assets\img\logo-linkin.png" height="39px" style="margin-bottom:-3px; margin-right: 10px"></a>
			<p> </p>
                        <a href="https://scholar.google.com/citations?user=aHJCt9oAAAAJ&hl=en"><img src="assets\img\logo-googlescholar.png" height="38px" style="margin-bottom:-3px; margin-right: 10px"></a>
                    </div>
                </div>
            </section>
            <hr class="m-0" />

            <!-- Research-->
            <section class="resume-section" id="research">
                <div class="resume-section-content">
                    <h2 class="mb-5">Research Projects</h2>
                        <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">3D Human Pose Reconstruction and Prediction from Wearable Sensors (<span style="color: brown">IEEE-JBHI</span> Minor Revision)</h3>
                            <div class="subheading mb-3"></div>
			    <!img src="assets\img\pose_model.png" alt="pose" width="1000" height="400">
			    <img src="assets\img\pose_model.png" alt="pose" style="width:90%">
			    <!img src="assets\img\pose_model.png" alt="pose" width="1000" height="400" style="width:100%">
                            <div class="subheading mb-3"></div>				
                            <p> <b>Abstract:</b> Reconstructing and predicting 3D human walking poses in unconstrained measurement environments have the potential to use for health monitoring systems for people with movement disabilities by assessing progression after treatments and providing information for assistive device controls. The latest pose estimation algorithms utilize motion capture systems, which capture data from IMU sensors and third-person view cameras. However, third-person views are not always possible for outpatients alone. Thus, we propose the wearable motion capture problem of reconstructing and predicting 3D human poses from the wearable IMU sensors and wearable cameras, which aids clinicians’ diagnoses on patients out of clinics. To solve this problem, we introduce a novel Attention-Oriented Recurrent Neural Network (AttRNet) that contains a sensor-wise attention-oriented recurrent encoder, a reconstruction module, and a dynamic temporal attention-oriented recurrent decoder, to reconstruct the 3D human pose over time and predict the 3D human poses at the following time steps. To evaluate our approach, we collected a new WearableMotionCapture dataset using wearable IMUs and wearable video cameras, along with the musculoskeletal joint angle ground truth. The proposed AttRNet shows high accuracy on the new lower-limbWearableMotionCapture dataset, and it also outperforms the state-of-the-art methods on two public full-body pose datasets: DIP-IMU and TotalCaputre.</p>
                            <div class="subheading mb-3"></div>	
			    <img src="assets\img\obstacle.gif" alt="pred1" style="width:80%">
			    <p> </p>
			    <!img src="assets\img\obstacle.gif" alt="pred1" style="width:100%">
			    
			    <p> </p>
			    <img src="assets\img\stair.gif" alt="pred2" style="width:80%">
			    <p> </p>
                            <div class="subheading mb-3"></div>	
			    <div class="subsubheading mb-0"><b> This research is supported by National Science Foundation (NSF)</b></div>
                        </div>
                        <!-- <div class="flex-shrink-0"><span class="text-primary">July 2010 - December 2011</span></div> -->
                    </div>
            <hr>

<!--  -----------------------------------------------------------------------------------------------------------------------                    -->

            <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Feature Weakening, Contextualization, and Discrimination for Weakly Supervised Temporal Action Localization (<span style="color: brown">IEEE-TMM 2023</span>) </h3>
                            <div class="subheading mb-3"></div>
                            <!img src="assets\img\overview_tal.png" alt="localization" style="width:100%" width="1000" height="300">
			    <img src="assets\img\overview_tal.png" alt="localization" style="width:90%">
                            <div class="subheading mb-3"></div>
                            <p> <b>Abstract:</b> Weakly-supervised Temporal Action Localization (W-TAL) aims to train a model to localize all action instances potentially from different classes in an untrimmed video, using a training dataset that has video-level action class labels but has no detailed annotations on the start and end timestamps of action instances. We propose to solve the W-TAL problem from the feature learning aspect, with a new architecture, termed F3-Net, which includes (1) a Feature Weakening (FW) module that can identify and randomly weaken either the most discriminative action or the most discriminative background features over the training iterations to force the network to precisely localize the action instances in both discriminative and ambiguous action-related frames, without spreading to the background intervals; (2) a Feature Contextualization (FC) module that can infer the global contexts among video segments and attentionally fuse them with the local contexts from individual video segments to generate more representative features; and (3) a Feature Discrimination (FD) module that can highlight the most discriminative video segments/classes corresponding to each class/segment, respectively, for localizing multiple action instances from different classes within a video. Experimental results on THUMOS14 and ActivityNet1.3 demonstrate the state-of-the-art performance of our F3-Net, and the FW and FC are also effective plug-in modules to improve other methods.</p>
                            <div class="subsubheading mb-0"><b> This research is supported by National Science Foundation (NSF)</b></div>
                        </div>
                        <!-- <div class="flex-shrink-0"><span class="text-primary">July 2010 - December 2011</span></div> -->
                    </div>
    <hr>	


<!--  -----------------------------------------------------------------------------------------------------------------------                    -->

            <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Collaborative Foreground, Background, and Action Modeling Network for Weakly Supervised Temporal Action Localization (IEEE-TCSVT 2023)</h3>
                            <div class="subheading mb-3"></div>
                            <!img src="assets\img\fba_net.png" alt="localization_tcsvt" width="1000" height="300" style="width:100%">
			    <img src="assets\img\fba_net.png" alt="localization_tcsvt" style="width:90%">
                            <div class="subheading mb-3"></div>
                            <p> <b>Abstract:</b> In this paper, we explore the problem of Weakly-supervised Temporal Action Localization (W-TAL), where the task is to localize the temporal boundaries of all action instances in an untrimmed video with only video-level supervision. The existing W-TAL methods achieve a good action localization performance by separating the discriminative action and background frames. However, there is still a large performance gap between the weakly and fully supervised methods. The main reason comes from that there are plenty of ambiguous action and background frames in addition to the discriminative action and background frames. Due to the lack of temporal annotations in W-TAL, the ambiguous background frames may be localized as foreground and the ambiguous action frames may be suppressed as background, which result in false positives and false negatives, respectively. In this paper, we introduce a novel collaborative Foreground, Background, and Action Modeling Network (FBANet) to suppress the background (i.e., both the discriminative and ambiguous background) frames, and localize the actual action-related (i.e., both the discriminative and ambiguous action) frames as foreground, for the precise temporal action localization. We design our FBA-Net with three branches: the foreground modeling (FM) branch, the background modeling (BM) branch, and the class-specific action and background modeling (CM) branch. The CM branch learns to highlight the video frames related to C action classes, and separate the action-related frames of C action classes from the (C + 1)th background class. The collaboration between FM and CM regularizes the consistency between the FM and the C action classes of CM, which reduces the false negative rate by localizing different actual-action-related (i.e., both the discriminative and ambiguous action) frames in a video as foreground. On the other hand, the collaboration between BM and CM regularizes the consistency between the BM and the (C + 1)th background class of CM, which reduces the false positive rate by suppressing both the discriminative and ambiguous background frames. Furthermore, the collaboration between FM and BM enforces more effective foreground-background separation. To evaluate the effectiveness of our FBA-Net, we perform extensive experiments on two challenging datasets, THUMOS14 and ActivityNet1.3. The experiments show that our FBA-Net attains superior results.</p>
				
			    <div class="subsubheading mb-0"><b> This research is supported by National Science Foundation (NSF)</b></div>
                        </div>
                        <!-- <div class="flex-shrink-0"><span class="text-primary">July 2010 - December 2011</span></div> -->
                    </div>
    <hr>


<!--  -----------------------------------------------------------------------------------------------------------------------                    -->

            <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Jointly-Learnt Networks for Future Action Anticipation via Self-Knowledge Distillation and Cycle Consistency (IEEE-TCSVT 2022)</h3>
                            <div class="subheading mb-3"></div>
                            <img src="assets\img\anticipation.png" alt="anticipation_tcsvt" height ="500">
                            <div class="subheading mb-3"></div>
                            <p> <b>Abstract:</b> Future action anticipation aims to infer future actions from the observation of a small set of past video frames. In this paper, we propose a novel Jointly-learnt Action Anticipation Network (J-AAN) via Self-Knowledge Distillation (Self-KD) and cycle consistency for future action anticipation. In contrast to the current state-of-the-art methods which anticipate the future actions either directly or recursively, our proposed J-AAN anticipates the future actions jointly in both direct and recursive ways. However, when dealing with future action anticipation, one important challenge to address is the future’s uncertainty since multiple action sequences may come from or be followed by the same action. Training an action anticipation model with one-hot-encoded hard labels that assign zero probabilities to incorrect yet semantically similar actions may not handle the uncertain future. To address this challenge, we design a Self-KD mechanism to train our J-AAN, where the J-AAN gradually distills its own knowledge during the training to soften the hard labels to model the uncertainty on future action anticipation. Furthermore, we design a forward and backward action anticipation framework with our proposed J-AAN based on a cyclic consistency constraint. The forward J-AAN anticipates the future actions from the observed past actions, and the backward J-AAN verifies the anticipation of the forward JAAN by anticipating the past actions from the anticipated future actions. The proposed method outperforms all the latest state-of-the-art action anticipation methods on the Breakfast, 50Salads, and EPIC-Kitchens-55 datasets. </p>
			    <div class="subsubheading mb-0"><b> This research is supported by National Science Foundation (NSF)</b></div>
                        </div>
                        <!-- <div class="flex-shrink-0"><span class="text-primary">July 2010 - December 2011</span></div> -->
                    </div>
    <hr>

<!--  -----------------------------------------------------------------------------------------------------------------------                    -->

            <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Human Action Recognition by Discriminative Feature Pooling and Video Segmentation Attention Model (IEEE-TMM 2021)</h3>
                            <div class="subheading mb-3"></div>
                            <img src="assets\img\overview_ar.png" alt="recognition" style="width:90%">
                            <div class="subheading mb-3"></div>
                            <p><b> Abstract: </b> We introduce a simple yet effective network that embeds a novel Discriminative Feature Pooling (DFP) mechanism and a novel Video Segment Attention Model (VSAM), for video-based human action recognition from both trimmed and untrimmed videos. Our DFP module introduces an attentional pooling mechanism for 3D Convolutional Neural Networks that attentionally pools 3D convolutional feature maps to emphasize the most critical spatial, temporal, and channel-wise features related to the actions within a video segment, while our VSAM ensembles these most critical features from all video segments and learns (1) class-specific attention weights to classify the video segments into the corresponding action categories, and (2) class-agnostic attention weights to rank the video segments based on their relevance to the action class. Our action recognition network can be trained from both trimmed videos in a fully-supervised way and untrimmed videos in a weakly-supervised way. For untrimmed videos with weak labels, our network learns attention weights without the requirement of precise temporal annotations of action occurrences in videos. Evaluated on the untrimmed video datasets of THUMOS14 and ActivityNet1.2, and trimmed video datasets of HMDB51, UCF101, and HOLLYWOOD2, our network achieves promising performance, compared to the latest state-of-the-art methods.</p>
                            	<div class="subsubheading mb-0"><b> This research is supported by National Science Foundation (NSF)</b></div>
                        </div>
                        <!-- <div class="flex-shrink-0"><span class="text-primary">July 2010 - December 2011</span></div> -->
                    </div>
    <hr>

<!--  -----------------------------------------------------------------------------------------------------------------------                    -->

            <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Action Completeness Modeling with Background Aware Networks for weakly-Supervised Temporal Action Localization (ACM-MM 2020)</h3>
                            <div class="subheading mb-3"></div>
                            <!img src="assets\img\banet.png" alt="localization_acmmm" style="width:100%" width = "1000" height ="350">
			    <img src="assets\img\banet.png" alt="localization_acmmm" style="width:90%">
                            <div class="subheading mb-3"></div>
                            <p> <b>Abstract:</b> The state-of-the-art of fully-supervised methods for temporal action localization from untrimmed videos has achieved impressive results. Yet, it remains unsatisfactory for the weakly-supervised temporal action localization, where only video-level action labels are given without the timestamp annotation on when the actions occur. The main reason comes from that, the weakly-supervised networks only focus on the highly discriminative frames, but there are some ambiguous frames in both background and action classes. The ambiguous frames in the background class are very similar to the real actions, which may be treated as target actions and result in false positives. On the other hand, the ambiguous frames in the action class which possibly contain action instances, are prone to be false negatives by the weakly-supervised networks and result in a coarse localization. To solve these problems, we introduce a novel weakly-supervised Action Completeness Modeling with Background Aware Networks (ACM-BANets). Our Background Aware Network (BANet) contains a weight-sharing two-branch architecture, with an action-guided Background aware Temporal Attention Module (B-TAM) and an asymmetrical training strategy, to suppress both highly discriminative and ambiguous background frames to remove the false positives. Our action completeness modeling contains multiple BANets, and the BANets are forced to discover different but complementary action instances to completely localize the action instances in both highly discriminative and ambiguous action frames. In the 𝑖-th iteration, the 𝑖-th BANet discovers the discriminative features, which are then erased from the feature map. The partially-erased feature map is fed into the (𝑖 + 1)-th BANet of the next iteration to force this BANet to discover discriminative features different from the 𝑖-th BANet. Evaluated on two challenging untrimmed video datasets, THUMOS14 and ActivityNet1.3, our approach outperforms all the current weakly-supervised methods for temporal action localization.</p>
				<div class="subsubheading mb-0"><b> This research is supported by National Science Foundation (NSF)</b></div>
                        </div>
                        <!-- <div class="flex-shrink-0"><span class="text-primary">July 2010 - December 2011</span></div> -->
                    </div>
    <hr>
			
<!--   --------------------------------------------------------------------------------------------------------------------------                   -->

                    <div class="d-flex flex-column flex-md-row justify-content-between">
                        <div class="flex-grow-1">
                            <h3 class="mb-0"> Spatial Attention Mechanism for Weakly Supervised Fire and Traffic Accident Scene Classification (SMARTCOMP 2019)</h3>
                            <div class="subheading mb-3"></div>
                            <img src="assets\img\scene_1.gif" alt="accident1" style="width:70%">
			    <p> </p>
			    <img src="assets\img\scene_2.gif" alt="accident2" style="width:70%">
                            <div class="subheading mb-3"></div>
                            <!-- <div class="subheading mb-3">Shout! Media Productions</div> -->
                            <p><b> Abstract: </b> During the past ten years, on average there were near 16.5 thousand of hazardous materials (hazmat) transport incidents per year resulting in $82 million of damages. Prompt, accurate, objective assessment on hazmat incidents is important for the first responders to take appropriate actions timely, which will reduce the damage of hazmat incidents and protect the safety of people and the environment. Therefore, one of the most important steps is to automatically detect transport incidents, such as fire and traffic accidents. In this paper, we introduce a simple and yet effective framework that integrates the convolutional feature maps of deep Convolutional Neural Networks with a spatial attention mechanism for fire and traffic accident scene classification. Our spatial attention model learns to highlight the most discriminative convolutional features, which is related to the regions of interest in the input image. We train our network in a weakly supervised way. In other words, without the requirement of a precise bounding box annotating the exact location of fire or traffic accidents in the image, our network can be learned from the only image-level label. In addition to the image-based traffic scene classification, the model is also applied on a set of collected videos for real-world applications. The proposed model, a simple end-to-end architecture, achieves promising performance on fire scene classification from images, and traffic accident scene classification from both images and videos.</p>
			    <div class="subsubheading mb-0"><b> This research is supported by Mid-America Transportation Center (MATC), and Intelligent Systems Center (ISC) at Missouri University of Science and Technology.</b></div>
                            <div class="subheading mb-5"></div>

                        </div>
                        <!-- <div class="flex-shrink-0"><span class="text-primary">September 2008 - June 2010</span></div> -->
                    </div>

                  </div>

            </section>
            <hr class="m-0" />


            <!-- Publications-->
            <section class="resume-section" id="publications">
                <div class="resume-section-content">
                    <h2 class="mb-5">Publications</h2>
                    <p> 1. <b>Md Moniruzzaman</b>, Zhaozheng Yin, Md Sanzid Bin Hossain, Hwan Choi, and Zhishan Guo. “Wearable Motion Capture: Reconstructing and Predicting 3D Human Poses from Wearable Sensors.” IEEE Journal of Biomedical and Health Informatics <b>(IEEE-JBHI)</b>, 2023 (Minor Revision).</p>
		    <p> 2. <b>Md Moniruzzaman</b>, and Zhaozheng Yin. “Feature Weakening, Contextualization, and Discrimination for Weakly Supervised Temporal Action Localization.” IEEE Transactions on Multimedia <b>(IEEE-TMM)</b>, Accepted, 2023. (Impact Factor: 8.182)</p>
		    <p> 3. <b>Md Moniruzzaman</b>, and Zhaozheng Yin. “Collaborative Foreground, Background, and Action Modeling Network for Weakly Supervised Temporal Action Localization.” IEEE Transactions on Circuits and Systems for Video Technology <b>(IEEE-TCSVT)</b>, Accepted, 2023. (Impact Factor: 5.859)</p>
		    <p> 4. <b>Md Moniruzzaman</b>, Md Moniruzzaman, Zhaozheng Yin, Zhihai He, Ruwen Qin, and Ming C Leu. “Jointly-Learnt Networks for Future Action Anticipation via Self-Knowledge Distillation and Cycle Consistency.” IEEE Transactions on Circuits and Systems for Video Technology <b>(IEEE-TCSVT)</b>, 2022. (Impact Factor: 5.859)</p>
		    <p> 5. <b>Md Moniruzzaman</b>, Zhaozheng Yin, Zhihai He, Ruwen Qin, and Ming C Leu. “Human Action Recognition by Discriminative Feature Pooling and Video Segmentation Attention Model.” IEEE Transactions on Multimedia <b>(IEEE-TMM)</b>, 2021. (Impact Factor: 8.182)</p>
		    <p> 6. Md Al-Amin, Ruwen Qin, <b>Md Moniruzzaman</b>, Zhaozheng Yin, Wenjin Tao, and Ming C Leu. “An individualized system of skeletal data-based CNN classifiers for action recognition in manufacturing assembly.” Journal of Intelligent Manufacturing, 2021.
		    <p> 7. <b>Md Moniruzzaman</b>, Zhaozheng Yin, Zhihai He, Ruwen Qin, and Ming C Leu. “Action Completeness Modeling with Background Aware Networks for Weakly-Supervised Temporal Action Localization.” In Proceedings of the 28th ACM International Conference on Multimedia <b>(ACM-MM)</b>, 2020.</p>
		    <p> 8. <b>Md Moniruzzaman</b>, Zhaozheng Yin, and Ruwen Qin. “Spatial Attention Mechanism for Weakly Supervised Fire and Traffic Accident Scene Classification.” In 2019 IEEE International Conference on Smart Computing, 2019.</p>
                </div>
            </section>
            <hr class="m-0" />


            <!-- Awards-->
            <section class="resume-section" id="awards">
                <div class="resume-section-content">
                    <h2 class="mb-5">Awards & Certifications</h2>
                    <ul class="fa-ul mb-0">
                        <li>
                            <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                            Academic Achievement Award, Department of Computer Science, Missouri University of Science and Technology, 2019
                        </li>
                        <li>
                            <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                            NSF Travel Grant Award in IEEE International Conference on Smart Computing, 2019.
                        </li>
	                <li>
                            <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                            Academic Achievement Award, Department of Computer Science, Missouri University of Science and Technology, 2018.
                        </li>
                        <li>
                            <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                            3
                            <sup>rd</sup>
                            Place - Best Poster Award, Poster Competition, Intelligent Systems Center (ISC), 2018.
                        </li>
                        <li>
                            <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                            2
                            <sup>nd</sup>
                            Place - Best Paper Award, Graduate Research Symposium, Intelligent Systems Center (ISC), 2018.
                        </li>
                    </ul>
                </div>
            </section>
            </section>
            <hr class="m-0" />


		

</html>
