<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" /> 
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>Md Moniruzzaman</title>
        <link rel="icon" type="image/x-icon" href="assets/img/favicon.ico" />
        <!-- Font Awesome icons (free version)-->
        <script src="https://use.fontawesome.com/releases/v5.15.1/js/all.js" crossorigin="anonymous"></script>
        <!-- Google fonts-->
        <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:500,700" rel="stylesheet" type="text/css" />
        <link href="https://fonts.googleapis.com/css?family=Muli:400,400i,800,800i" rel="stylesheet" type="text/css" />


        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="css/styles.css" rel="stylesheet" />
    </head>
    <body id="page-top">
        <!-- Navigation-->
        <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
            <a class="navbar-brand js-scroll-trigger" href="#page-top">
                <span class="d-block d-lg-none">Md Moniruzzaman</span>
<!--                 <span class="d-none d-lg-block"><img class="img-fluid img-profile rounded-circle mx-auto mb-2" src="assets/img/profile.jpg" alt="" /></span> -->
                <span class="d-none d-lg-block"><img class="img-fluid img-profile mx-auto mb-2" src="assets/img/website.jpg" alt="" /></span>
            </a>
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button>
            <div class="collapse navbar-collapse" id="navbarSupportedContent">
                <ul class="navbar-nav">
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#about">About</a></li>
                    <!-- <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#education">Education</a></li> -->
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#research">Research</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#publications">Publications</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#Teaching">Teaching</a></li>
                    <!-- <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#skills">Skills</a></li> -->
                    <!-- <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#talk">Invited Talk</a></li> -->
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#awards">Awards</a></li>
                    <!-- <li class="nav-item"><a class="nav-link js-scroll-trigger" href="https://drive.google.com/file/d/1jcq1fAVDR_diUBetd99vsOKdnsZk8W2j/view" target="_blank">CV</a></li> -->
                    <!li class="nav-item"><a class="nav-link js-scroll-trigger" href="#blog">blog</a></li>
                    <!li class="nav-item"><a class="nav-link js-scroll-trigger" href="demo/gallery.html">Gallery</a></li>
                    <!-- <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#gallery">Gallery</a></li> -->
                </ul>
            </div>
        </nav>
        <!-- Page Content-->
        <div class="container-fluid p-0">
            <!-- About-->
            <section class="resume-section" id="about">
                <div class="resume-section-content">
                    <h1 class="mb-0">
                        Md
                        <span class="text-primary">Moniruzzaman</span>
                    </h1>
                    <div class="subheading mb-5">
                        Stony Brook University · New York 11794 · USA · (573) 476-4683 ·
                        <a href="mailto:mmoniruzzama@cs.stonybrook.edu">mmoniruzzama@cs.stonybrook.edu</a>
                    </div>                     
                      
                
                    <p class="lead mb-5">
		     I am a Ph.D. student in the Department of Computer Science at Stony Brook University working with <a href="https://www.cs.stonybrook.edu/people/faculty/ZhaozhengYin">Dr. Zhaozheng Yin</a>. I am interested in designing and implementing machine learning and deep learning-based algorithms to solve computer vision problems, such as image and video classification. In particular, I am familiar with video-based human action recognition, temporal action localization, future action anticipation, human pose estimation, and repetitive action counting with deep learning models.</p>				

                    <div class="social-icons">
                        <a href="https://www.linkedin.com/in/md-moniruzzaman-1730441a3/"><img src="assets\img\logo-linkin.png" height="39px" style="margin-bottom:-3px; margin-right: 10px"></a>
                        <a href="https://scholar.google.com/citations?user=aHJCt9oAAAAJ&hl=en"><img src="assets\img\logo-googlescholar.png" height="38px" style="margin-bottom:-3px; margin-right: 10px"></a>
                    </div>
                </div>
            </section>
            <hr class="m-0" />

            <!-- Research-->
            <section class="resume-section" id="research">
                <div class="resume-section-content">
                    <h2 class="mb-5">Research Projects</h2>
                        <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">3D Human Pose Reconstruction and Prediction from Wearable Sensors</h3>
                            <div class="subheading mb-3"></div>
			    <img src="assets\img\pose_model.png" alt="pose" width="900" height="480">	
                            <div class="subheading mb-3"></div>				
                            <p> <b>Abstract:</b> Reconstructing and predicting 3D human walking poses in unconstrained measurement environments have the potential to use for health monitoring systems for people with movement disabilities by assessing progression after treatments and providing information for assistive device controls. The latest pose estimation algorithms utilize motion capture systems, which capture data from IMU sensors and third-person view cameras. However, third-person views are not always possible for outpatients alone. Thus, we propose the wearable motion capture problem of reconstructing and predicting 3D human poses from the wearable IMU sensors and wearable cameras, which aids clinicians’ diagnoses on patients out of clinics. To solve this problem, we introduce a novel Attention-Oriented Recurrent Neural Network (AttRNet) that contains a sensor-wise attention-oriented recurrent encoder, a reconstruction module, and a dynamic temporal attention-oriented recurrent decoder, to reconstruct the 3D human pose over time and predict the 3D human poses at the following time steps. To evaluate our approach, we collected a new WearableMotionCapture dataset using wearable IMUs and wearable video cameras, along with the musculoskeletal joint angle ground truth. The proposed AttRNet shows high accuracy on the new lower-limbWearableMotionCapture dataset, and it also outperforms the state-of-the-art methods on two public full-body pose datasets: DIP-IMU and TotalCaputre.</p>
                            <div class="subheading mb-3"></div>	
			    <img src="assets\img\obstacle.gif" alt="pred1">
			    <p>                                 Avoiding obstacles </p>
			    <img src="assets\img\stair.gif" alt="pred2">
			    <p>                                 Walking on Stair </p>
                            <div class="subheading mb-3"></div>	
			    <div class="subsubheading mb-0"><b> This research is supported by National Science Foundation (NSF)</b></div>
                        </div>
                        <!-- <div class="flex-shrink-0"><span class="text-primary">July 2010 - December 2011</span></div> -->
                    </div>
            <hr>

<!--  -----------------------------------------------------------------------------------------------------------------------                    -->

            <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Feature Weakening, Contextualization, and Discrimination for Weakly Supervised Temporal Action Localization</h3>
                            <div class="subheading mb-3"></div>
                            <img src="assets\img\overview_tal.png" alt="localization" width="1000" height="380">
                            <div class="subheading mb-3"></div>
                            <p> <b>Abstract:</b> Weakly-supervised Temporal Action Localization (W-TAL) aims to train a model to localize all action instances potentially from different classes in an untrimmed video, using a training dataset that has video-level action class labels but has no detailed annotations on the start and end timestamps of action instances. We propose to solve the W-TAL problem from the feature learning aspect, with a new architecture, termed F3-Net, which includes (1) a Feature Weakening (FW) module that can identify and randomly weaken either the most discriminative action or the most discriminative background features over the training iterations to force the network to precisely localize the action instances in both discriminative and ambiguous action-related frames, without spreading to the background intervals; (2) a Feature Contextualization (FC) module that can infer the global contexts among video segments and attentionally fuse them with the local contexts from individual video segments to generate more representative features; and (3) a Feature Discrimination (FD) module that can highlight the most discriminative video segments/classes corresponding to each class/segment, respectively, for localizing multiple action instances from different classes within a video. Experimental results on THUMOS14 and ActivityNet1.3 demonstrate the state-of-the-art performance of our F3-Net, and the FW and FC are also effective plug-in modules to improve other methods.</p>
                            <div class="subsubheading mb-0"><b> This resarch is supported by National Science Foundation (NSF)</b></div>
                        </div>
                        <!-- <div class="flex-shrink-0"><span class="text-primary">July 2010 - December 2011</span></div> -->
                    </div>
    <hr>	


<!--  -----------------------------------------------------------------------------------------------------------------------                    -->

            <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Collaborative Foreground, Background, and Action Modeling Network for Weakly Supervised Temporal Action Localization</h3>
                            <div class="subheading mb-3"></div>
                            <img src="assets\img\fba_net.png" alt="localization_tcsvt" width="1024">
                            <div class="subheading mb-3"></div>
                            <p> <b>Abstract:</b> In this paper, we explore the problem of Weakly-supervised Temporal Action Localization (W-TAL), where the task is to localize the temporal boundaries of all action instances in an untrimmed video with only video-level supervision. The existing W-TAL methods achieve a good action localization performance by separating the discriminative action and background frames. However, there is still a large performance gap between the weakly and fully supervised methods. The main reason comes from that there are plenty of ambiguous action and background frames in addition to the discriminative action and background frames. Due to the lack of temporal annotations in W-TAL, the ambiguous background frames may be localized as foreground and the ambiguous action frames may be suppressed as background, which result in false positives and false negatives, respectively. In this paper, we introduce a novel collaborative Foreground, Background, and Action Modeling Network (FBANet) to suppress the background (i.e., both the discriminative and ambiguous background) frames, and localize the actual action-related (i.e., both the discriminative and ambiguous action) frames as foreground, for the precise temporal action localization. We design our FBA-Net with three branches: the foreground modeling (FM) branch, the background modeling (BM) branch, and the class-specific action and background modeling (CM) branch. The CM branch learns to highlight the video frames related to C action classes, and separate the action-related frames of C action classes from the (C + 1)th background class. The collaboration between FM and CM regularizes the consistency between the FM and the C action classes of CM, which reduces the false negative rate by localizing different actual-action-related (i.e., both the discriminative and ambiguous action) frames in a video as foreground. On the other hand, the collaboration between BM and CM regularizes the consistency between the BM and the (C + 1)th background class of CM, which reduces the false positive rate by suppressing both the discriminative and ambiguous background frames. Furthermore, the collaboration between FM and BM enforces more effective foreground-background separation. To evaluate the effectiveness of our FBA-Net, we perform extensive experiments on two challenging datasets, THUMOS14 and ActivityNet1.3. The experiments show that our FBA-Net attains superior results.</p>
				
			    <div class="subsubheading mb-0"><b> This resarch is supported by National Science Foundation (NSF)</b></div>
                        </div>
                        <!-- <div class="flex-shrink-0"><span class="text-primary">July 2010 - December 2011</span></div> -->
                    </div>
    <hr>
			
<!--   --------------------------------------------------------------------------------------------------------------------------                   -->

                    <div class="d-flex flex-column flex-md-row justify-content-between">
                        <div class="flex-grow-1">
                            <h3 class="mb-0"> Human Action Recognition by Discriminative Feature Pooling and Video Segmentation Attention Model</h3>
                            <div class="subheading mb-3"></div>
                            <img src="assets\img\overview_ar.png" alt="recognition" width="1024">
                            <div class="subheading mb-3"></div>
                            <!-- <div class="subheading mb-3">Shout! Media Productions</div> -->
                            <p><b> Abstract: </b> We introduce a simple yet effective network that embeds a novel Discriminative Feature Pooling (DFP) mechanism and a novel Video Segment Attention Model (VSAM), for video-based human action recognition from both trimmed and untrimmed videos. Our DFP module introduces an attentional pooling mechanism for 3D Convolutional Neural Networks that attentionally pools 3D convolutional feature maps to emphasize the most critical spatial, temporal, and channel-wise features related to the actions within a video segment, while our VSAM ensembles these most critical features from all video segments and learns (1) class-specific attention weights to classify the video segments into the corresponding action categories, and (2) class-agnostic attention weights to rank the video segments based on their relevance to the action class. Our action recognition network can be trained from both trimmed videos in a fully-supervised way and untrimmed videos in a weakly-supervised way. For untrimmed videos with weak labels, our network learns attention weights without the requirement of precise temporal annotations of action occurrences in videos. Evaluated on the untrimmed video datasets of THUMOS14 and ActivityNet1.2, and trimmed video datasets of HMDB51, UCF101, and HOLLYWOOD2, our network achieves promising performance, compared to the latest state-of-the-art methods.</p>
                            <div class="subsubheading mb-0"><b> This resarch is supported by National Science Foundation (NSF)</b></div>
                            <div class="subheading mb-5"></div>

                        </div>
                        <!-- <div class="flex-shrink-0"><span class="text-primary">September 2008 - June 2010</span></div> -->
                    </div>

                  </div>

            </section>
            <hr class="m-0" />


            <!-- Publications-->
            <section class="resume-section" id="publications">
                <div class="resume-section-content">
                    <h2 class="mb-5">Publications</h2>
                    <p> 1. <b>Md Moniruzzaman</b>, Zhaozheng Yin, Md Sanzid Bin Hossain, Hwan Choi, and Zhishan Guo. “Wearable Motion Capture: Reconstructing and Predicting 3D Human Poses from Wearable Sensors.” IEEE Journal of Biomedical and Health Informatics <b>(IEEE-JBHI)</b>, 2023 (Minor Revision).</p>
		    <p> 2. <b>Md Moniruzzaman</b>, and Zhaozheng Yin. “Feature Weakening, Contextualization, and Discrimination for Weakly Supervised Temporal Action Localization.” IEEE Transactions on Multimedia <b>(IEEE-TMM)</b>, Accepted, 2023. (Impact Factor: 8.182)</p>
		    <p> 3. <b>Md Moniruzzaman</b>, and Zhaozheng Yin. “Collaborative Foreground, Background, and Action Modeling Network for Weakly Supervised Temporal Action Localization.” IEEE Transactions on Circuits and Systems for Video Technology <b>(IEEE-TCSVT)</b>, Accepted, 2023. (Impact Factor: 5.859)</p>
		    <p> 4. <b>Md Moniruzzaman</b>, Md Moniruzzaman, Zhaozheng Yin, Zhihai He, Ruwen Qin, and Ming C Leu. “Jointly-Learnt Networks for Future Action Anticipation via Self-Knowledge Distillation and Cycle Consistency.” IEEE Transactions on Circuits and Systems for Video Technology <b>(IEEE-TCSVT)</b>, 2022. (Impact Factor: 5.859)</p>
		    <p> 5. <b>Md Moniruzzaman</b>, Zhaozheng Yin, Zhihai He, Ruwen Qin, and Ming C Leu. “Human Action Recognition by Discriminative Feature Pooling and Video Segmentation Attention Model.” IEEE Transactions on Multimedia <b>(IEEE-TMM)</b>, 2021. (Impact Factor: 8.182)</p>
		    <p> 6. Md Al-Amin, Ruwen Qin, <b>Md Moniruzzaman</b>, Zhaozheng Yin, Wenjin Tao, and Ming C Leu. “An individualized system of skeletal data-based CNN classifiers for action recognition in manufacturing assembly.” Journal of Intelligent Manufacturing, 2021.
		    <p> 7. <b>Md Moniruzzaman</b>, Zhaozheng Yin, Zhihai He, Ruwen Qin, and Ming C Leu. “Action Completeness Modeling with Background Aware Networks for Weakly-Supervised Temporal Action Localization.” In Proceedings of the 28th ACM International Conference on Multimedia <b>(ACM-MM)</b>, 2020.</p>
		    <p> 8. <b>Md Moniruzzaman</b>, Zhaozheng Yin, and Ruwen Qin. “Spatial Attention Mechanism for Weakly Supervised Fire and Traffic Accident Scene Classification.” In 2019 IEEE International Conference on Smart Computing, 2019.</p>
                </div>
            </section>
            <hr class="m-0" />

</html>
